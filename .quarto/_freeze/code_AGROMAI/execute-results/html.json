{
  "hash": "d164f3ca38007444f2b7d39046dcfd8e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Teste Técnico\nsubtitle: '**Posição Júnior - Agromai**'\nauthor: Fernanda Kelly R. Silva | www.fernandakellyrs.com\neditor: visual\nlang: pt\ndate: 27/01/2025\ndate-format: short\ntoc: true\nformat:\n  html:\n    embed-resources: true\n    code-fold: false\n    code-tools: true\n    theme:\n      light: cosmo\n      dark: superhero\ntitle-block-banner: 'true'\ncode-annotations: hover\nexecute:\n  warning: false\n  message: false\n---\n\n# Instalação de pacotes\n\nAo instalar os pacotes no início de qualquer projeto, garantimos o acesso a um ecossistema padronizado e poderoso, eliminando a necessidade de procurar soluções fragmentadas para diferentes partes do fluxo de trabalho, o que acelera o processo de desenvolvimento e reduz erros.\n\nTodas as bibliotecas enviadas como necessárias para instalação foram instaladas via terminal utlizando *pip install nome-pacote==version*.\n\nÉ importante ressaltar que estou utilizando a IDE RStudio. Essa IDE é passivel de programação em python através do pacote [reticulate](https://cran.r-project.org/web/packages/reticulate/index.html). Como interface estou utilizando o [Quarto](https://quarto.org/docs/get-started/hello/rstudio.html).\n\n\n```{markdown}\nR version 4.4.2 (2024-10-31) -- \"Pile of Leaves\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu\n```\n\n```{r}\nlibrary(tidyverse)\nlibrary(reticulate)\nlibrary(readr)\n```\n\n::: {#c1924282 .cell execution_count=1}\n``` {.python .cell-code}\n# pandas==1.3.3\n# numpy==1.21.2\n# scipy==1.7.1\n# matplotlib==3.4.3\n# scikit-learn==0.24.2\n# statsmodels==0.12.2\n\nimport pandas\nimport numpy\nimport scipy\nimport matplotlib\nimport sklearn\nimport statsmodels\n```\n:::\n\n\n# Question 1: *Data Cleaning and Basic Analysis*\n\nWork with sales data to demonstrate your ability to handle missing values and perform basic statistical analysis.\n\n::: panel-tabset\n# Python\n\n1.  Load and examine the dataset\n\nFazendo upload da base de dados.\n\n::: {#1ff280bf .cell execution_count=2}\n``` {.python .cell-code}\nsales_data = pandas.read_csv(\"technical-interview/data/sales_data.csv\")\ncustomer_purchases = pandas.read_csv(\"technical-interview/data/customer_purchases.csv\")\n```\n:::\n\n\nVerifiquei que não há chave entre as bases para um futuro join.\n\n2.  Clean the data:\n\n    -   Handle missing values in the 'price' column (replace with mean)\n    -   Handle missing values in the 'category' column (replace with mode)\n\nÉ possível notar que há 30 valores ausentes na coluna *category*. Como é solicitado, é necessário inserir a moda (mode), que pela frequência é a categoria *Electronics*, visto que a definição de moda é aquela categoria/número que mais se repete.\n\n::: {#9990ff43 .cell execution_count=3}\n``` {.python .cell-code}\nfrequency_table = sales_data['category'].value_counts(dropna=False).reset_index()\nprint(frequency_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        category  count\n0    Electronics    208\n1  Home & Garden    201\n2         Sports    197\n3       Clothing    185\n4          Books    179\n5            NaN     30\n```\n:::\n:::\n\n\nNo caso da variável **price**, utilizarei a função *describe*, visto que essa variável é numérica continua. Logo, podemos concluir que há 49 valores ausentes, sua média é de 101.88 e demais informações de 1°,2°, 3° quartil, minimo e máximo.\n\n::: {#a8bcfcae .cell execution_count=4}\n``` {.python .cell-code}\nsummary_table = sales_data['price'].describe()\nsummary_table\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\ncount    951.000000\nmean     101.883052\nstd       30.379179\nmin       12.359485\n25%       81.635403\n50%      101.314344\n75%      121.246977\nmax      195.793227\nName: price, dtype: float64\n```\n:::\n:::\n\n\nO número de valores ausentes é:\n\n::: {#3c9707e8 .cell execution_count=5}\n``` {.python .cell-code}\nvalor_na = sales_data['price'].isna().sum()\nvalor_na\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nnp.int64(49)\n```\n:::\n:::\n\n\nPara que não haja erro de consultas futuramente, irei criar uma nova variável com essas alterações. Essa variável será denominada por *category_Mode* e, para variável *price* como *price_Mean*. Sendo assim,\n\n::: {#36f5d113 .cell execution_count=6}\n``` {.python .cell-code}\nMean_price = sales_data['price'].mean()\nsales_data['price_Mean']  = sales_data['price'].fillna(Mean_price)\n\nMode_category = sales_data['category'].mode()[0]\nsales_data['category_Mode']  = sales_data['category'].fillna(Mode_category)\n```\n:::\n\n\nAvaliando os NA's, veja que não há mais valores ausentes.\n\n::: {#1d849516 .cell execution_count=7}\n``` {.python .cell-code}\nsales_data['price_Mean'].isna().sum()\nsales_data['category_Mode'].isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nnp.int64(0)\n```\n:::\n:::\n\n\n-   Convert 'sale_date' to datetime\n\nEm relação a variável *sale_date* temos que esta pertence a classe *Date*, para que esta seja *datetime*,\n\n::: {#2b578741 .cell execution_count=8}\n``` {.python .cell-code}\nsales_data['sale_date'] = pandas.to_datetime(sales_data['sale_date'])\n```\n:::\n\n\n3.  Create a summary of:\n\nPara as tabelas resumo estou considerando as variáveis *sem* os valores ausentes.\n\n-   Total number of sales per category\n\n::: {#93acadcf .cell execution_count=9}\n``` {.python .cell-code}\ntotalNumber_category = sales_data.groupby('category_Mode').size()\ntotalNumber_category\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\ncategory_Mode\nBooks            179\nClothing         185\nElectronics      238\nHome & Garden    201\nSports           197\ndtype: int64\n```\n:::\n:::\n\n\n-   Average price per category\n\nPara *Average price per category*:\n\n::: {#4484b41a .cell execution_count=10}\n``` {.python .cell-code}\naveragePrice_category = sales_data.groupby('category_Mode').mean('price')\naveragePrice_category\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sale_id</th>\n      <th>price</th>\n      <th>quantity</th>\n      <th>price_Mean</th>\n    </tr>\n    <tr>\n      <th>category_Mode</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Books</th>\n      <td>497.307263</td>\n      <td>101.690430</td>\n      <td>5.117318</td>\n      <td>101.695811</td>\n    </tr>\n    <tr>\n      <th>Clothing</th>\n      <td>508.735135</td>\n      <td>103.822460</td>\n      <td>5.118919</td>\n      <td>103.707144</td>\n    </tr>\n    <tr>\n      <th>Electronics</th>\n      <td>505.848739</td>\n      <td>100.468345</td>\n      <td>4.743697</td>\n      <td>100.533731</td>\n    </tr>\n    <tr>\n      <th>Home &amp; Garden</th>\n      <td>467.104478</td>\n      <td>102.661415</td>\n      <td>5.228856</td>\n      <td>102.614946</td>\n    </tr>\n    <tr>\n      <th>Sports</th>\n      <td>523.279188</td>\n      <td>101.188330</td>\n      <td>5.025381</td>\n      <td>101.223595</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n-   Number of missing values handled\n\nPara *Number of missing values handled*, vimos que manipulamos um total de 79 caselas.\n\n::: {#de08131b .cell execution_count=11}\n``` {.python .cell-code}\nsales_data['price'].isna().sum()\nsales_data['category'].isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nnp.int64(30)\n```\n:::\n:::\n\n\n# R\n\nFazendo upload da base de dados.\n\n\n```{r}\ncustomer_purchases_R <- readr::read_csv(\"technical-interview/data/customer_purchases.csv\")\nsales_data_R <- readr::read_csv(\"technical-interview/data/sales_data.csv\")\n```\n\n\nÉ possível notar que há 30 valores ausentes na coluna *category*. Como é solicitado, é necessário inserir a moda (mode), que pela frequência é a categoria *Electronics*, visto que a definição de moda é aquela categoria/número que mais se repete.\n\n\n```{r}\njanitor::tabyl(sales_data_R$category, show_na = TRUE)\n```\n\n\nNo caso da variável **price**, utilizarei a função *summary*, visto que essa variável é numérica continua. Logo, podemos concluir que há 49 valores ausentes, sua média é de 101.88 e demais informações de 1°,2°, 3° quartil, minimo e máximo.\n\n\n```{r}\nsummary(sales_data_R$price)\n```\n\n\nPara que não haja erro de consultas futuramente, irei criar uma nova variável com essas alterações. Essa variável será denominada por *category_Mode* e, para variável *price* como *price_Mean*. Sendo assim,\n\n\n```{r}\nsales_data_R <- sales_data_R %>% \n  dplyr::mutate(category_Mode = base::ifelse(is.na(category), \"Electronics\", category),\n                price_Mean = base::ifelse(is.na(price), base::mean(price, na.rm = TRUE), price))\n```\n\n\nAvaliando a variável *category_Mode*:\n\n\n```{r}\njanitor::tabyl(sales_data_R$category_Mode, show_na = TRUE)\n```\n\n\nAvaliando a variável *price_Mean*:\n\n\n```{r}\nsummary(sales_data_R$price_Mean)\n```\n\n\nVeja que não há mais valores ausentes.\n\nEm relação a variável *sale_date* temos que esta pertence a classe *Date*, para que esta seja *datetime*,\n\n\n```{r}\nsales_data_R <- sales_data_R %>% \n  dplyr::mutate(sale_date_datetime = as.POSIXct(sale_date))\n```\n\n\nAvaliando,\n\n\n```{r}\nclass(sales_data_R$sale_date_datetime)\n```\n\n\nPara as tabelas resumo estou considerando as variáveis *sem* os valores ausentes,\n\nPara *Total number of sales per category* foi considerado *sale_id* como indicador para o número de vendas. Veja abaixo o resultado.\n\n\n```{r}\nsales_data_R %>% \n  dplyr::group_by(category_Mode) %>% \n  dplyr::count() \n```\n\n\nPara *Average price per category*:\n\n\n```{r}\nsales_data_R %>% \n  dplyr::group_by(category_Mode) %>% \n  dplyr::summarise(average_price =  mean(price_Mean))\n```\n\n\nPara *Number of missing values handled*, vimos que manipulamos um total de 79 caselas.\n:::\n\n# Question 2: Data Manipulation and Aggregation:\n\nShow your skills in transforming and aggregating data to derive meaningful insights.\n\n::: panel-tabset\n# Python\n\n1.  Calculate the following metrics per customer:\n\nO *per customer* é um pouco ambíguo e, por isso, abaixo estão as medidas em sua totalidade e agrupadas por *customer_id*.\n\n-   Total amount spent\n\n::: {#b33fe673 .cell execution_count=12}\n``` {.python .cell-code}\namountSpent = customer_purchases['amount'].sum().round(2)\namountSpent\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nnp.float64(101993.11)\n```\n:::\n:::\n\n\n::: {#e45c88ad .cell execution_count=13}\n``` {.python .cell-code}\namountSpent_PC = customer_purchases.groupby('customer_id', as_index=False)['amount'].sum()\namountSpent_PC\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CUST_001</td>\n      <td>1550.75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CUST_002</td>\n      <td>1313.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CUST_003</td>\n      <td>1323.92</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CUST_004</td>\n      <td>1102.69</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CUST_005</td>\n      <td>1199.27</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>CUST_096</td>\n      <td>1382.41</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>CUST_097</td>\n      <td>958.23</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>CUST_098</td>\n      <td>646.45</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>CUST_099</td>\n      <td>2000.90</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>CUST_100</td>\n      <td>887.31</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\n-   Average purchase value\n\n::: {#e65153a3 .cell execution_count=14}\n``` {.python .cell-code}\namountMean = customer_purchases['amount'].mean()\namountMean\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nnp.float64(101.99311)\n```\n:::\n:::\n\n\n::: {#f171efc5 .cell execution_count=15}\n``` {.python .cell-code}\namountMean_PC = customer_purchases.groupby('customer_id', as_index=False)['amount'].mean()\namountMean_PC\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CUST_001</td>\n      <td>96.921875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CUST_002</td>\n      <td>101.057692</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CUST_003</td>\n      <td>110.326667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CUST_004</td>\n      <td>100.244545</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CUST_005</td>\n      <td>109.024545</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>CUST_096</td>\n      <td>106.339231</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>CUST_097</td>\n      <td>106.470000</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>CUST_098</td>\n      <td>92.350000</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>CUST_099</td>\n      <td>111.161111</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>CUST_100</td>\n      <td>98.590000</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\n-   Number of purchases\n\nO número de compras é o número total de observações do banco de dados, visto que não há *purchase_id* com repetição. Porém, para cada *customer_id* teremos os seguintes resultados:\n\n::: {#da6b3652 .cell execution_count=16}\n``` {.python .cell-code}\namountCount_PC = customer_purchases.groupby('customer_id', as_index=False)['purchase_id'].count()\namountCount_PC\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>purchase_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CUST_001</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CUST_002</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CUST_003</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CUST_004</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CUST_005</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>CUST_096</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>CUST_097</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>CUST_098</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>CUST_099</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>CUST_100</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\nLogo, se consideramos o número de compras por customer, teremos somente 100 compradores.\n\n::: {#666c3d83 .cell execution_count=17}\n``` {.python .cell-code}\namountCount = customer_purchases.groupby('customer_id').size()\namountCount\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\ncustomer_id\nCUST_001    16\nCUST_002    13\nCUST_003    12\nCUST_004    11\nCUST_005    11\n            ..\nCUST_096    13\nCUST_097     9\nCUST_098     7\nCUST_099    18\nCUST_100     9\nLength: 100, dtype: int64\n```\n:::\n:::\n\n\n-   Most frequently bought category\n\nA categoria mais frequente é a *Home & Garden*.\n\n::: {#e42c5b3b .cell execution_count=18}\n``` {.python .cell-code}\nfreqBought_category = customer_purchases.groupby('category').size()\nfreqBought_category\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ncategory\nBooks            182\nClothing         186\nElectronics      202\nHome & Garden    225\nSports           205\ndtype: int64\n```\n:::\n:::\n\n\n2.  Create a summary DataFrame with:\n\n-   Top 5 customers by total spend\n\n::: {#a793afbc .cell execution_count=19}\n``` {.python .cell-code}\ntop5_customer = (customer_purchases.groupby('customer_id')\n          .agg(somaAmount=('amount', 'sum'))  # Calculando a soma da coluna 'amount'\n          .reset_index()  # Para garantir que 'customer_id' seja uma coluna normal\n          .sort_values(by='somaAmount', ascending=False))  # Ordenando pela soma em ordem decrescente\n\ntop5_customer\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>somaAmount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>61</th>\n      <td>CUST_062</td>\n      <td>2102.22</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>CUST_090</td>\n      <td>2040.24</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>CUST_099</td>\n      <td>2000.90</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>CUST_092</td>\n      <td>1779.33</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>CUST_033</td>\n      <td>1775.29</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>CUST_091</td>\n      <td>480.91</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>CUST_040</td>\n      <td>459.16</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>CUST_083</td>\n      <td>450.57</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CUST_011</td>\n      <td>373.26</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>CUST_031</td>\n      <td>356.43</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\n-   Bottom 5 customers by total spend\n\n::: {#06c0a6d5 .cell execution_count=20}\n``` {.python .cell-code}\ntop5_customer = (customer_purchases.groupby('customer_id')\n          .agg(somaAmount=('amount', 'sum'))  # Calculando a soma da coluna 'amount'\n          .reset_index()  # Para garantir que 'customer_id' seja uma coluna normal\n          .sort_values(by='somaAmount', ascending=True))  # Ordenando pela soma em ordem crescente\n\ntop5_customer\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>somaAmount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30</th>\n      <td>CUST_031</td>\n      <td>356.43</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>CUST_011</td>\n      <td>373.26</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>CUST_083</td>\n      <td>450.57</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>CUST_040</td>\n      <td>459.16</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>CUST_091</td>\n      <td>480.91</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>CUST_033</td>\n      <td>1775.29</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>CUST_092</td>\n      <td>1779.33</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>CUST_099</td>\n      <td>2000.90</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>CUST_090</td>\n      <td>2040.24</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>CUST_062</td>\n      <td>2102.22</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\n3.  Calculate the monthly purchase trends:\n\nAqui irei utilizar o banco de dados sales_data.\n\n-   Total sales per month\n\nÉ necessário fazer a tratativa de datas.\n\n::: {#7306fab3 .cell execution_count=21}\n``` {.python .cell-code}\nsales_data['sale_date'] = pandas.to_datetime(sales_data['sale_date'])\n```\n:::\n\n\nComo o interesse da medida é por mês, a ideia é extrair da data somente o mês.\n\n::: {#d2627beb .cell execution_count=22}\n``` {.python .cell-code}\nsales_data['mes_Sale'] = sales_data['sale_date'].dt.month\n```\n:::\n\n\nHá, desde o início, a prerrogativa de trataiva dos valores ausente da variável *price* com sua respectiva média, sendo assim:\n\n::: {#9a5d7fa8 .cell execution_count=23}\n``` {.python .cell-code}\nmean_price = sales_data['price'].mean() \nsales_data['price'] = sales_data['price'].fillna(mean_price) \n```\n:::\n\n\nÉ importante ressaltar que não há em escrito que a variável price seja por unidade, mas como há a variável *quantity* e para um bom analista, tira-se por base que a variável *price* é por unidade. Sendo assim, devemos multiplicar o preço pelo número de unidades para alcançar o valor real gasto na compra.\n\n::: {#c5f45950 .cell execution_count=24}\n``` {.python .cell-code}\nsales_data['sales'] = sales_data['price']*sales_data['quantity']\n```\n:::\n\n\nA solicitação é o valor gasto por mês, logo há a necessidade de agrupar por mês o valor total gasto na compra.\n\n::: {#e2081ae0 .cell execution_count=25}\n``` {.python .cell-code}\nsales_data['totalSales'] = sales_data.groupby('mes_Sale')['sales'].transform('sum')\n```\n:::\n\n\nA seguir a tabela resultado:\n\n::: {#1578e2d0 .cell execution_count=26}\n``` {.python .cell-code}\ndistinct_sales_data = sales_data.drop_duplicates(subset=['mes_Sale'])\nprint(distinct_sales_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     sale_id  sale_date       category       price  quantity  price_Mean  \\\n0          1 2023-01-01  Home & Garden  101.883052         6  101.883052   \n31        32 2023-02-01  Home & Garden   98.371154         1   98.371154   \n59        60 2023-03-01       Clothing   76.665499         3   76.665499   \n90        91 2023-04-01          Books   61.587119         1   61.587119   \n120      121 2023-05-01       Clothing  107.680892         2  107.680892   \n151      152 2023-06-01    Electronics  153.124019         9  153.124019   \n181      182 2023-07-01          Books  120.445021         5  120.445021   \n212      213 2023-08-01          Books   87.394395         4   87.394395   \n243      244 2023-09-01          Books  118.987955         8  118.987955   \n273      274 2023-10-01  Home & Garden  133.261107         3  133.261107   \n304      305 2023-11-01          Books  132.208952         2  132.208952   \n334      335 2023-12-01  Home & Garden   92.762918         5   92.762918   \n\n     category_Mode  mes_Sale        sales    totalSales  \n0    Home & Garden         1   611.298313  47466.125884  \n31   Home & Garden         2    98.371154  42919.491076  \n59        Clothing         3   229.996498  43805.601813  \n90           Books         4    61.587119  46116.775448  \n120       Clothing         5   215.361784  45649.176138  \n151    Electronics         6  1378.116172  49171.638869  \n181          Books         7   602.225105  50650.744834  \n212          Books         8   349.577582  47388.221534  \n243          Books         9   951.903636  44419.928465  \n273  Home & Garden        10   399.783322  32468.332365  \n304          Books        11   264.417905  29878.150269  \n334  Home & Garden        12   463.814591  31286.220835  \n```\n:::\n:::\n\n\n-   Average purchase value per month\n\nComo este caso é para a compra, seguiremos a mesma lógica da questão anterior, porém com o banco de dados *customer*. É importante ressaltar que no caso da variável *amount* não temos valores faltantes.\n\n::: {#dc9653bb .cell execution_count=27}\n``` {.python .cell-code}\ncustomer_purchases['purchase_date'] = pandas.to_datetime(customer_purchases['purchase_date'])\ncustomer_purchases['mes_Purchase'] = customer_purchases['purchase_date'].dt.month\n\ncustomer_purchases['totalCustomer'] = customer_purchases.groupby('mes_Purchase')['amount'].transform('mean')\n\ndistinct_customer_data = customer_purchases.drop_duplicates(subset=['mes_Purchase'])\nprint(distinct_customer_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     purchase_id customer_id purchase_date       category  amount  \\\n0             52    CUST_004    2023-01-01    Electronics  108.36   \n122          230    CUST_097    2023-02-01          Books   91.00   \n207           82    CUST_044    2023-03-01         Sports  125.50   \n308          656    CUST_018    2023-04-02       Clothing   94.02   \n409          856    CUST_079    2023-05-01  Home & Garden   72.28   \n507          373    CUST_099    2023-06-01         Sports   97.69   \n620          275    CUST_001    2023-07-01         Sports  135.30   \n700          947    CUST_094    2023-08-01         Sports  113.58   \n813          382    CUST_097    2023-09-01          Books  126.37   \n899          669    CUST_027    2023-10-01       Clothing  115.95   \n\n     mes_Purchase  totalCustomer  \n0               1     102.720328  \n122             2     104.238824  \n207             3     101.432079  \n308             4     100.293267  \n409             5     101.720102  \n507             6     103.816018  \n620             7     101.929250  \n700             8     102.948850  \n813             9     100.950814  \n899            10      99.579802  \n```\n:::\n:::\n\n\n-   Bonus: Identify any customers who haven't made a purchase in the last 3 months\n\nUtilizando a data de 27/01/2025 como data diária, no banco de dados oferecido todos os clientes não fizeram compras nos últimos 3 meses. Devido a este caso, vou utilizar a data máxima do banco de dados que é 2023-10-27 como referência. Sendo assim, teremos *5* clientes que não compram há mais de 3 meses.\n\n-   CUST_041\n-   CUST_048\n-   CUST_031\n-   CUST_016\n-   CUST_067\n\nPara chegar a esta conclusão utilizei os seguintes passo:\n\n-   Criando a coluna com data especificada e inserindo no banco de dados:\n\n::: {#eed30eb5 .cell execution_count=28}\n``` {.python .cell-code}\ndata_estudo = pandas.to_datetime(\"2023-10-27\")\ncustomer_purchases['data_estudo'] = data_estudo\n```\n:::\n\n\n-   Para que haja a dioferença de datas, ambas precisam estar na mesma classe:\n\n::: {#f78b7536 .cell execution_count=29}\n``` {.python .cell-code}\ncustomer_purchases['data_estudo'] = pandas.to_datetime(customer_purchases['data_estudo'])\ncustomer_purchases['purchase_date'] = pandas.to_datetime(customer_purchases['purchase_date'])\n```\n:::\n\n\n-   Fazendo a diferença entre as datas e contabilizando o número de dias, lembrando que o nosso interesse é em contar aqueles que não compram a mais de 3 meses (90 dias).\n\n::: {#dccd33fa .cell execution_count=30}\n``` {.python .cell-code}\ncustomer_purchases['puchase_Day'] = (customer_purchases['purchase_date'] - customer_purchases['data_estudo']).dt.days\n```\n:::\n\n\n-   Se o interesse é naqueles que não compram a mais de 90 dias, pensei em criar uma dummy com SIM e NÃO para aqueles que não compram a -90 dias, ou seja, o meu interesse é por aquele que será classificado como SIM.\n\n::: {#bea002b9 .cell execution_count=31}\n``` {.python .cell-code}\ncustomer_purchases['puchase_Day3'] = customer_purchases['puchase_Day'].apply(lambda x: 'Sim' if x < -90 else 'Não')\n```\n:::\n\n\n-   Esse é o momento crucial para a análise, visto que o mesmo customer poderá estar classificado como SIM e NÃO, visto que este customer pode fazer mais de uma compra, inclusive, de produtos diferentes. Ou seja, ele não compra há mais de 90 dias aquele determinado produto, mas comprou outros, o que faz que ele não se encaixe nessa categoria de clientes que não compram há mais de 90 dias.\n\n::: {#166b6ef9 .cell execution_count=32}\n``` {.python .cell-code}\nbonusCustomer = customer_purchases.groupby('customer_id')['puchase_Day3'].value_counts().reset_index(name='count')\nprint(bonusCustomer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    customer_id puchase_Day3  count\n0      CUST_001          Sim     12\n1      CUST_001          Não      4\n2      CUST_002          Sim     10\n3      CUST_002          Não      3\n4      CUST_003          Sim      9\n..          ...          ...    ...\n190    CUST_098          Não      1\n191    CUST_099          Não     10\n192    CUST_099          Sim      8\n193    CUST_100          Sim      5\n194    CUST_100          Não      4\n\n[195 rows x 3 columns]\n```\n:::\n:::\n\n\n# R\n\n1.  Calculate the following metrics per customer:\n\n    -   Total amount spent\n\n\n```{r}\nbase::sum(customer_purchases_R$amount)\n```\n\n\n-   Average purchase value\n\n\n```{r}\nbase::mean(customer_purchases_R$amount)\n```\n\n\n-   Number of purchases\n\nO número de compras é o número total de observações, visto que não há *purchase_id* com repetição.\n\n\n```{r}\ncustomer_purchases %>% \n  dplyr::count(purchase_id) \n```\n\n\nPorém, se consideramos o número de compras por customer, teremos somente 100 compras.\n\n\n```{r}\ncustomer_purchases %>% \ndplyr::group_by(customer_id) %>% \ndplyr::count()\n```\n\n\n-   Most frequently bought category\n\n\n```{r}\ncustomer_purchases_R %>% \n  dplyr::group_by(category) %>% \n  dplyr::count()\n```\n\n\n2.  Create a summary DataFrame with:\n\n    -   Top 5 customers by total spend\n\n\n```{r}\ncustomer_purchases_R %>% \n  dplyr::group_by(customer_id) %>% \n  dplyr::summarise(meanCustomer = base::mean(amount)) %>% \n  dplyr::arrange(desc(meanCustomer))\n```\n\n\n-   Bottom 5 customers by total spend\n\n\n```{r}\ncustomer_purchases_R %>% \n  dplyr::group_by(customer_id) %>% \n  dplyr::summarise(meanCustomer = base::mean(amount)) %>% \n  dplyr::arrange((meanCustomer))\n```\n\n\n3.  Calculate the monthly purchase trends:\n\n    -   Total sales per month\n\n\n```{r}\n# sales_data_R <- sales_data_R %>% \n#   dplyr::mutate(mes_Sale = lubridate::month(sale_date)) %>% \n#   dplyr::mutate(price = base::ifelse(is.na(price), base::mean(price, na.rm = TRUE), price),\n#                 sales = quantity*price) %>% \n#   dplyr::group_by(mes_Sale) %>% \n#   dplyr::mutate(totalSales = sum(sales)) %>% \n#   dplyr::ungroup()\n\nsales_data_R %>% \n  dplyr::mutate(mes_Sale = lubridate::month(sale_date)) %>% \n  dplyr::mutate(price = base::ifelse(is.na(price), base::mean(price, na.rm = TRUE), price),\n                sales = quantity*price) %>% \n  dplyr::group_by(mes_Sale) %>% \n  dplyr::mutate(totalSales = sum(sales)) \n```\n\n\n-   Average purchase value per month\n\n\n```{r}\ncustomer_purchases_R %>% \n  dplyr::mutate(mes_Purchase = lubridate::month(purchase_date)) %>% \n  dplyr::group_by(mes_Purchase) %>% \n  dplyr::mutate(totalPurchase = mean(amount)) \n```\n\n\n-   Bonus: Identify any customers who haven't made a purchase in the last 3 months\n\nUtilizando a data de 27/01/2025 como data diária, no banco de dados oferecido todos os clientes não fizeram compras nos últimos 3 meses. Devido a este caso, vou utilizar a data máxima do banco de dados que é 2023-10-27 como referência. Sendo assim, teremos *5* clientes que não compram há mais de 3 meses.\n\n-   CUST_041\n-   CUST_048\n-   CUST_031\n-   CUST_016\n-   CUST_067\n\n\n```{r}\ncustomer_purchases_R %>% \n  dplyr::mutate(data_estudo = rep(\"2023-10-27\", 1000),\n                data_estudo      = base::as.Date(data_estudo),\n                puchase_Day = purchase_date - data_estudo,\n                puchase_Day3 = base::ifelse(puchase_Day < -90, \"Sim\", \"Não\")) %>% \n  dplyr::group_by(customer_id) %>%\n  dplyr::count(puchase_Day3)\n```\n\n:::\n\n# Question 3: Data Visualization:\n\nCreate informative visualizations to communicate data insights effectively.\n\n1.  Create a line plot showing daily sales trends over time\n    -   Include a 7-day moving average line\n\nPara a construção desse gráfico, vou seguir as seguintes etapas:\n\n-   Como vamos trabalhar com data, ela precisa estar na classe desejada.\n\n::: {#a63e8fdb .cell execution_count=33}\n``` {.python .cell-code}\nsales_data['sale_date'] = pandas.to_datetime(sales_data['sale_date'])\n```\n:::\n\n\n-   O objetivo é que esse plot seja diário de vendas, por isso, independente do produto, devemos agrupar a quantidade de venda por dia, mas nós não temos mais de uma venda por dia, o que temos variando é a quantidade de vendas, o que faz sentido ter uma média móvel.\n\n::: {#06925484 .cell execution_count=34}\n``` {.python .cell-code}\ndiaSales = sales_data.groupby('sale_date')['quantity'].sum().reset_index()\n```\n:::\n\n\n-   Ordenando as datas devido a média a móvel:\n\n::: {#652942b9 .cell execution_count=35}\n``` {.python .cell-code}\ndiaSales = diaSales.sort_values(by='sale_date')\n```\n:::\n\n\n-   Calculando a média móvel de 7 dias:\n\n::: {#d9bee1d0 .cell execution_count=36}\n``` {.python .cell-code}\ndiaSales['7Dias_avg'] = diaSales['quantity'].rolling(window=7).mean()\n```\n:::\n\n\n-   Plotando o gráfico:\n\n::: {#8096da38 .cell execution_count=37}\n``` {.python .cell-code}\n# matplotlib.pyplot.figure(figsize=(12, 6))\n# \n# matplotlib.pyplot.plot()(diaSales['sale_date'], diaSales['quantity'], label='Vendas Diárias', color='blue', alpha=0.6)\n# matplotlib.pyplot.plot(diaSales['sale_date'], diaSales['7Dias_avg'], label='Média Móvel (7 dias)', color='red', linewidth=2)\n```\n:::\n\n\n2.  Create a bar plot showing:\n\n    -   Total sales by category\n    -   Include error bars representing standard deviation\n    \n\n```{r}\nggplot(sales_data_R) +\n  aes(x = category_Mode, weight = sales) +\n  geom_bar(fill = \"#112446\") +\n  theme_minimal()\n```\n\n    \n\n3.  Create a scatter plot showing:\n\n    -   Relationship between quantity and price\n    -   Color points by category\n    -   Add a trend line\n\n\n```{r}\nggplot(sales_data_R) +\n  aes(x = quantity, y = price, colour = category_Mode) +\n  geom_jitter(size = 1.5) +\n  scale_color_hue(direction = 1) +\n  theme_minimal()\n```\n\n\nRequirements: \n- Use appropriate labels and titles\n- Include a legend where necessary \n- Use a consistent color scheme \n- Save all plots as PNG files\n\n# Question 4: Advanced Analytics (Required for Data Scientists, Bonus for Juniors):\n\nApply advanced statistical methods and machine learning techniques to solve a complex business problem.\n\n1.  Implement error handling for the following analyses:\n\n    -   Perform a one-way ANOVA test to compare prices across different categories\n    -   Calculate and plot the confidence intervals for mean prices in each category\n    -   Identify potential outliers using z-scores\n\n2.  Debug and fix the following code snippet that attempts to perform a chi-square test:\n\n``` python\n    def perform_chi_square(data):\n        observed = data.groupby(['category', 'status']).size()\n        chi2, p_value = stats.chi2_contingency(observed)\n        return chi2, p_value\n```\n\n3.  Implement proper logging to track:\n\n    -   Any statistical assumptions violations\n    -   Data type mismatches\n    -   Invalid calculations\n\nYour solution should be robust against various edge cases and include appropriate error messages.\n\n",
    "supporting": [
      "code_AGROMAI_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}